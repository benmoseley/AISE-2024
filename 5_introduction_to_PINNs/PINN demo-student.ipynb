{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e966a723",
   "metadata": {},
   "source": [
    "# Physics-informed neural network (PINN) demo\n",
    "\n",
    "In this demo we will code a PINN from scratch in `PyTorch` and use it to solve simulation and inversion problems related to the damped harmonic oscillator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee1bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b64298",
   "metadata": {},
   "source": [
    "## Problem overview\n",
    "\n",
    "We are going to use a PINN to solve problems related to the **damped harmonic oscillator**:\n",
    "\n",
    "<img src=\"oscillator.gif\" width=\"500\">\n",
    "\n",
    "We are interested in modelling the displacement of the mass on a spring (green box) over time.\n",
    "\n",
    "This is a canonical physics problem, where the displacement, $u(t)$, of the oscillator as a function of time can be described by the following differential equation:\n",
    "\n",
    "$$\n",
    "m \\dfrac{d^2 u}{d t^2} + \\mu \\dfrac{d u}{d t} + ku = 0~,\n",
    "$$\n",
    "\n",
    "where $m$ is the mass of the oscillator, $\\mu$ is the coefficient of friction and $k$ is the spring constant.\n",
    "\n",
    "We will focus on solving the problem in the **under-damped state**, i.e. where the oscillation is slowly damped by friction (as displayed in the animation above). \n",
    "\n",
    "Mathematically, this occurs when:\n",
    "\n",
    "$$\n",
    "\\delta < \\omega_0~,~~~~~\\mathrm{where}~~\\delta = \\dfrac{\\mu}{2m}~,~\\omega_0 = \\sqrt{\\dfrac{k}{m}}~.\n",
    "$$\n",
    "\n",
    "Furthermore, we consider the following initial conditions of the system:\n",
    "\n",
    "$$\n",
    "u(t=0) = 1~~,~~\\dfrac{d u}{d t}(t=0) = 0~.\n",
    "$$\n",
    "\n",
    "For this particular case, the exact solution is known and given by:\n",
    "\n",
    "$$\n",
    "u(t) = e^{-\\delta t}(2 A \\cos(\\phi + \\omega t))~,~~~~~\\mathrm{with}~~\\omega=\\sqrt{\\omega_0^2 - \\delta^2}~.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "For a more detailed mathematical description of the harmonic oscillator, check out this blog post: https://beltoforion.de/en/harmonic_oscillator/.\n",
    "\n",
    "## Workflow overview\n",
    "\n",
    "There are **two scientific tasks** related to the harmonic oscillator we will use a PINN for:\n",
    "\n",
    ">First, we will **simulate** the system using a PINN, given its initial conditions.\n",
    "\n",
    ">Second, we will **invert** for underlying parameters of the system using a PINN, given some noisy observations of the oscillator's displacement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d82ee71",
   "metadata": {},
   "source": [
    "## Initial setup\n",
    "\n",
    "First, we define a few helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac19c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_solution(d, w0, t):\n",
    "    \"Defines the analytical solution to the under-damped harmonic oscillator problem above.\"\n",
    "    assert d < w0\n",
    "    w = np.sqrt(w0**2-d**2)\n",
    "    phi = np.arctan(-d/w)\n",
    "    A = 1/(2*np.cos(phi))\n",
    "    cos = torch.cos(phi+w*t)\n",
    "    exp = torch.exp(-d*t)\n",
    "    u = exp*2*A*cos\n",
    "    return u\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    \"Defines a fully-connected network in PyTorch\"\n",
    "    def __init__(self, N_INPUT, N_OUTPUT, N_HIDDEN, N_LAYERS):\n",
    "        super().__init__()\n",
    "        activation = nn.Tanh\n",
    "        self.fcs = nn.Sequential(*[\n",
    "                        nn.Linear(N_INPUT, N_HIDDEN),\n",
    "                        activation()])\n",
    "        self.fch = nn.Sequential(*[\n",
    "                        nn.Sequential(*[\n",
    "                            nn.Linear(N_HIDDEN, N_HIDDEN),\n",
    "                            activation()]) for _ in range(N_LAYERS-1)])\n",
    "        self.fce = nn.Linear(N_HIDDEN, N_OUTPUT)\n",
    "    def forward(self, x):\n",
    "        x = self.fcs(x)\n",
    "        x = self.fch(x)\n",
    "        x = self.fce(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54deb812",
   "metadata": {},
   "source": [
    "## Task 1: train a PINN to simulate the system\n",
    "\n",
    "#### Task\n",
    "\n",
    "The first task is to use a PINN to **simulate** the system.\n",
    "\n",
    "Specifically, our inputs and outputs are:\n",
    "\n",
    "- Inputs: underlying differential equation and the initial conditions of the system\n",
    "- Outputs: estimate of the solution, $u(t)$\n",
    "\n",
    "#### Approach\n",
    "\n",
    "The PINN is trained to directly approximate the solution to the differential equation, i.e.\n",
    "\n",
    "$$\n",
    "N\\!N(t;\\theta) \\approx u(t)~,\n",
    "$$\n",
    "\n",
    "For this task, we use $\\delta=2$, $\\omega_0=20$, $m=1$, and try to learn the solution over the domain $t\\in [0,1]$.\n",
    "\n",
    "#### Loss function\n",
    "\n",
    "To simulate the system, the PINN is trained with the following loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta)= (N\\!N(0;\\theta) - 1)^2 + \\lambda_1 \\left(\\frac{d N\\!N}{dt}(0;\\theta) - 0\\right)^2 + \\frac{\\lambda_2}{N} \\sum^{N}_{i} \\left( \\left[ m\\frac{d^2}{dt^2} + \\mu \\frac{d}{dt} + k \\right] N\\!N(t_{i};\\theta)  \\right)^2\n",
    "$$\n",
    "\n",
    "#### Computing gradients\n",
    "\n",
    "To compute gradients of the neural network with respect to its inputs, we will use `torch.autograd.grad`:\n",
    "\n",
    "<img src=\"autograd_grad.png\" width=\"800\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f9321b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# define a neural network to train\n",
    "pinn = FCN(1,1,32,3)\n",
    "\n",
    "# define boundary points, for the boundary loss\n",
    "# TODO # (1, 1)\n",
    "\n",
    "# define training points over the entire domain, for the physics loss\n",
    "# TODO # (30, 1)\n",
    "\n",
    "# train the PINN\n",
    "d, w0 = 2, 20\n",
    "mu, k = 2*d, w0**2\n",
    "t_test = torch.linspace(0,1,300).view(-1,1)\n",
    "u_exact = exact_solution(d, w0, t_test)\n",
    "optimiser = torch.optim.Adam(pinn.parameters(),lr=1e-3)\n",
    "for i in range(15001):\n",
    "    optimiser.zero_grad()\n",
    "    \n",
    "    # compute each term of the PINN loss function above\n",
    "    # using the following hyperparameters\n",
    "    lambda1, lambda2 = 1e-1, 1e-4\n",
    "    \n",
    "    # compute boundary loss\n",
    "    # TODO\n",
    "\n",
    "    \n",
    "    # compute physics loss\n",
    "    # TODO\n",
    "\n",
    "    \n",
    "    # backpropagate joint loss, take optimiser step\n",
    "    loss = loss1 + lambda1*loss2 + lambda2*loss3\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    \n",
    "    # plot the result as training progresses\n",
    "    if i % 5000 == 0: \n",
    "        #print(u.abs().mean().item(), dudt.abs().mean().item(), d2udt2.abs().mean().item())\n",
    "        u = pinn(t_test).detach()\n",
    "        plt.figure(figsize=(6,2.5))\n",
    "        plt.scatter(t_physics.detach()[:,0], \n",
    "                    torch.zeros_like(t_physics)[:,0], s=20, lw=0, color=\"tab:green\", alpha=0.6)\n",
    "        plt.scatter(t_boundary.detach()[:,0], \n",
    "                    torch.zeros_like(t_boundary)[:,0], s=20, lw=0, color=\"tab:red\", alpha=0.6)\n",
    "        plt.plot(t_test[:,0], u_exact[:,0], label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n",
    "        plt.plot(t_test[:,0], u[:,0], label=\"PINN solution\", color=\"tab:green\")\n",
    "        plt.title(f\"Training step {i}\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b680afed",
   "metadata": {},
   "source": [
    "## Task 2: train a PINN to invert for underlying parameters\n",
    "\n",
    "#### Task\n",
    "\n",
    "The second task is to use a PINN to **invert** for underlying parameters.\n",
    "\n",
    "Specifically, our inputs and outputs are:\n",
    "\n",
    "- Inputs: noisy observations of the oscillator's displacement, $u_{\\mathrm{obs}}$\n",
    "- Outputs: estimate $\\mu$, the coefficient of friction\n",
    "\n",
    "#### Approach\n",
    "\n",
    "Similar to above, the PINN is trained to directly approximate the solution to the differential equation, i.e.\n",
    "\n",
    "$$\n",
    "N\\!N(t;\\theta) \\approx u(t)~,\n",
    "$$\n",
    "\n",
    "However here we assume $\\mu$ is **not known** and we treat it as an additional **learnable parameter** when training the PINN.\n",
    "\n",
    "#### Loss function\n",
    "\n",
    "The PINN is trained with the loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\mu)= \\frac{1}{N} \\sum^{N}_{i} \\left( \\left[ m\\frac{d^2}{dt^2} + \\mu \\frac{d}{dt} + k \\right] N\\!N(t_{i};\\theta)  \\right)^2 + \\frac{\\lambda}{M} \\sum^{M}_{j} \\left( N\\!N(t_{j};\\theta) - u_{\\mathrm{obs}}(t_{j}) \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5871be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create some noisy observational data\n",
    "torch.manual_seed(123)\n",
    "d, w0 = 2, 20\n",
    "print(f\"True value of mu: {2*d}\")\n",
    "t_obs = torch.rand(40).view(-1,1)\n",
    "u_obs = exact_solution(d, w0, t_obs) + 0.04*torch.randn_like(t_obs)\n",
    "t_test = torch.linspace(0,1,300).view(-1,1)\n",
    "u_exact = exact_solution(d, w0, t_test)\n",
    "\n",
    "plt.figure(figsize=(6,2.5))\n",
    "plt.title(\"Noisy observational data\")\n",
    "plt.scatter(t_obs[:,0], u_obs[:,0])\n",
    "plt.plot(t_test[:,0], u_exact[:,0], label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df8a1a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# define a neural network to train\n",
    "pinn = FCN(1,1,32,3)\n",
    "\n",
    "# define training points over the entire domain, for the physics loss\n",
    "t_physics = torch.linspace(0,1,30).view(-1,1).requires_grad_(True)# (30, 1)\n",
    "\n",
    "# train the PINN\n",
    "d, w0 = 2, 20\n",
    "_, k = 2*d, w0**2\n",
    "t_test = torch.linspace(0,1,300).view(-1,1)\n",
    "u_exact = exact_solution(d, w0, t_test)\n",
    "\n",
    "# treat mu as a learnable parameter, add it to optimiser\n",
    "# TODO\n",
    "\n",
    "\n",
    "mus = []\n",
    "for i in range(15001):\n",
    "    optimiser.zero_grad()\n",
    "    \n",
    "    # compute each term of the PINN loss function above\n",
    "    # using the following hyperparameters\n",
    "    lambda1 = 1e4\n",
    "    \n",
    "    # compute physics loss\n",
    "    u = pinn(t_physics)# (30, 1)\n",
    "    dudt = torch.autograd.grad(u, t_physics, torch.ones_like(u), create_graph=True)[0]# (30, 1)\n",
    "    d2udt2 = torch.autograd.grad(dudt, t_physics, torch.ones_like(dudt), create_graph=True)[0]# (30, 1)\n",
    "    loss1 = torch.mean((d2udt2 + mu*dudt + k*u)**2)\n",
    "    \n",
    "    # compute data loss\n",
    "    # TODO\n",
    "\n",
    "    \n",
    "    # backpropagate joint loss, take optimiser step\n",
    "    loss = loss1 + lambda1*loss2\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    \n",
    "    # record mu value\n",
    "    mus.append(mu.item())\n",
    "    \n",
    "    # plot the result as training progresses\n",
    "    if i % 5000 == 0: \n",
    "        u = pinn(t_test).detach()\n",
    "        plt.figure(figsize=(12,2.5))\n",
    "        \n",
    "        plt.subplot(1,2,1)\n",
    "        plt.scatter(t_obs[:,0], u_obs[:,0], label=\"Noisy observations\", alpha=0.6, color=\"tab:blue\")\n",
    "        plt.plot(t_test[:,0], u[:,0], label=\"PINN solution\", color=\"tab:green\")\n",
    "        plt.title(f\"Training step {i}\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1,2,2)\n",
    "        plt.title(\"$\\mu$\")\n",
    "        plt.plot(mus, label=\"PINN estimate\", color=\"tab:green\")\n",
    "        plt.hlines(2*d, 0, len(mus), label=\"True value\", color=\"tab:grey\")\n",
    "        plt.xlabel(\"Training step\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd85665f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
